% Generated by roxygen2: do not edit by hand
% Please edit documentation in _main.Rmd.
\name{css}
\alias{css}
\title{Cluster Stability Selection}
\usage{
css(
  X,
  y,
  lambda,
  clusters = list(),
  fitfun = cssLasso,
  sampling_type = "SS",
  B = ifelse(sampling_type == "MB", 100L, 50L),
  prop_feats_remove = 0,
  train_inds = integer(),
  num_cores = 1L
)
}
\arguments{
\item{X}{An n x p numeric matrix (preferably) or a data.frame (which will
be coerced internally to a matrix by the function model.matrix) containing
p >= 2 features/predictors.}

\item{y}{The response; can be anything that takes the form of an
n-dimensional vector, with the ith entry corresponding to the ith row of X.
Typically (and for default \code{fitfun = cssLasso}), \code{y} should be an n-dimensional
numeric vector.}

\item{lambda}{A tuning parameter or set of tuning parameters that may be used
by the feature selection method \code{fitfun}. In the default case when
\code{fitfun = cssLasso}, lambda should be a numeric: the penalty to use for each
lasso fit. (\code{css()} does not require lambda to be any particular object because
for a user-specified feature selection method \code{fitfun}, lambda can be an
arbitrary object. See the description of \code{fitfun} below.)}

\item{clusters}{A list of integer vectors; each vector should contain the
indices of a cluster of features (a subset of \code{1:p}). (If there is only one
cluster, clusters can either be a list of length 1 or an integer vector.)
All of the provided clusters must be non-overlapping. Every feature not
appearing in any cluster will be assumed to be unclustered (that is, they
will be treated as if they are in a "cluster" containing only themselves). If
clusters is a list of length 0 (or a list only containing clusters of length
1), then \code{css()} returns the same results as stability selection (so the
returned \code{feat_sel_mat} will be identical to \code{clus_sel_mat}). Names for the
clusters will be needed later; any clusters that are not given names in the
provided list will be given names automatically by \code{css()}. Default is
\code{list()} (so no clusters are specified).}

\item{fitfun}{A function; the feature selection function used on each
subsample by cluster stability selection. This can be any feature selection
method; the only requirement is that it accepts the arguments (and only the
arguments) \code{X}, \code{y}, and \code{lambda} and returns an integer vector that is a
subset of \code{1:p}. For example, \code{fitfun} could be best subset selection or
forward stepwise selection or LARS and \code{lambda} could be the desired model
size; or \code{fitfun} could be the elastic net and \code{lambda} could be a length-two
vector specifying lambda and alpha. Default is \code{cssLasso}, an implementation
of lasso (relying on the R package \code{glmnet}), where \code{lambda} must be a
positive numeric specifying the L1 penalty for the \code{lasso}.}

\item{sampling_type}{A character vector; either "SS" or "MB". For "MB",
all B subsamples are drawn randomly (as proposed by Meinshausen and Bühlmann
2010). For "SS", in addition to these B subsamples, the B complementary pair
subsamples will be drawn as well (see Faletto and Bien 2022 or Shah and
Samworth 2013 for details). Default is "SS", and "MB" is not supported yet.}

\item{B}{Integer or numeric; the number of subsamples. Note: For
\code{sampling_type=="MB"} the total number of subsamples will be \code{B}; for
\code{sampling_type="SS"} the number of subsamples will be \code{2*B}. Default is 100
for \code{sampling_type="MB"} and 50 for \code{sampling_type="SS"}.}

\item{prop_feats_remove}{Numeric; if \code{prop_feats_remove} is greater than 0,
then on each subsample, each feature is randomly dropped from the design
matrix that is provided to \code{fitfun} with probability \code{prop_feats_remove}
(independently across features). That is, in a typical subsample,
\code{prop_feats_remove*p} features will be dropped (though this number will vary).
This is similar in spirit (but distinct from) extended stability selection
(Beinrucker et. al. 2016); see their paper for some of the benefits of
dropping features (besides increasing computational speed and decreasing
memory requirements). For \code{sampling_type="SS"}, the features dropped in
each complementary pair of subsamples are identical in order to ensure that
the theoretical guarantees of Faletto and Bien (2022) are retained within
each individual pair of subsamples. (Note that this feature is not
investigated either theoretically or in simulations by Faletto and Bien
2022). Must be between 0 and 1. Default is 0.}

\item{train_inds}{Optional; an integer or numeric vector containing the
indices of observations in \code{X} and \code{y} to set aside for model training by the
function \code{getCssPreds()} after feature selection. (This will only work if \code{y} is
real-valued, because \code{getCssPreds()} using ordinary least squares regression to
generate predictions.) If \code{train_inds} is not provided, all of the observations
in the provided data set will be used for feature selection.}

\item{num_cores}{Optional; an integer. If using parallel processing, the
number of cores to use for parallel processing (\code{num_cores} will be supplied
internally as the \code{mc.cores} argument of \code{parallel::mclapply()}).}
}
\value{
A list containing the following items:
\item{\code{feat_sel_mat}}{A \code{B} (or \code{2*B} for \code{sampling_type="SS"}) x \code{p} numeric (binary) matrix. \code{feat_sel_mat[i, j] = 1} if feature \code{j} was selected by the base feature selection method on subsample \code{i}, and 0 otherwise.}
\item{\code{clus_sel_mat}}{A \code{B} (or \code{2*B} for SS sampling) x \code{length(clusters)} numeric (binary) matrix. \code{clus_sel_mat[i, j] = 1} if at least one feature from cluster j was selected by the base feature selection method on subsample \code{i}, and 0 otherwise.}
\item{\code{X}}{The \code{X} matrix provided to \code{css()}, coerced from a data.frame to a
matrix if needed.}
\item{\code{y}}{The \code{y} vector provided to \code{css()}.}
\item{\code{clusters}}{A named list of integer vectors containing all of the clusters provided to \code{css()}, as well as size 1 clusters of any features not listed in any
of the clusters provided to \code{css()}. All clusters will have names; any
clusters not provided with a name in the input to \code{css()} will be given names
automatically by \code{css()} (of the form c1, etc.).}
\item{\code{train_inds}}{Identical to the \code{train_inds} provided to \code{css()}.}
}
\description{
Executes cluster stability selection algorithm. Takes subsamples of data,
executes feature selection algorithm on each subsample, and returns matrices
of feature selection indicators as well as cluster selection indicators.
}
\references{
Faletto, G., & Bien, J. (2022). Cluster Stability Selection.
\emph{arXiv preprint arXiv:2201.00494}.
\url{https://arxiv.org/abs/2201.00494}.

Shah, R. D., & Samworth, R. J.
(2013). Variable selection with error control: Another look at stability
selection. \emph{Journal of the Royal Statistical Society. Series B:
Statistical Methodology}, 75(1), 55–80.
\url{https://doi.org/10.1109/RITA.2014.2302071}.

Meinshausen, N., & Bühlmann, P. (2010). Stability Selection. \emph{Journal of the Royal
Statistical Society. Series B: Statistical Methodology}, 72(4), 417–473.
\url{https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2010.00740.x}.

Beinrucker, A., Dogan, Ü., &
Blanchard, G. (2016). Extensions of stability selection using subsamples of
observations and covariates. \emph{Statistics and Computing}, 26(5), 1059-
1077. \url{https://doi.org/10.1007/s11222-015-9589-y}.

Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized
Linear Models via Coordinate Descent. \emph{Journal of Statistical Software},
33(1), 1-22. URL \url{https://www.jstatsoft.org/v33/i01/}.
}
\author{
Gregory Faletto, Jacob Bien
}